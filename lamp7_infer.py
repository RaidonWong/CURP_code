import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
from tqdm import tqdm
import argparse
from typing import List, Dict, Any


class PQCodebookModel(nn.Module):
    """PQ Codebookæ¨¡å‹ï¼ˆç”¨äºStage 2ï¼Œåªç”¨äºé‡åŒ–ï¼Œä¸è®­ç»ƒï¼‰"""
    def __init__(self, codebook_path, device="cpu"):
        super().__init__()
        # åŠ è½½è®­ç»ƒå¥½çš„PQ codebook
        checkpoint = torch.load(codebook_path, map_location=device)
        
        if "codebooks" in checkpoint:
            # å°†codebooksè½¬æ¢ä¸ºParameterList
            codebooks_list = []
            for cb in checkpoint["codebooks"]:
                if isinstance(cb, torch.Tensor):
                    codebooks_list.append(nn.Parameter(cb.to(device), requires_grad=False))
                else:
                    codebooks_list.append(nn.Parameter(torch.tensor(cb, device=device), requires_grad=False))
            
            self.codebooks = nn.ParameterList(codebooks_list)
            self.num_subspaces = checkpoint.get("num_subspaces", len(self.codebooks))
            self.subspace_dim = checkpoint.get("subspace_dim", self.codebooks[0].shape[1] if len(self.codebooks) > 0 else None)
            self.codebook_size = self.codebooks[0].shape[0]
            self.emb_dim = self.num_subspaces * self.subspace_dim
        else:
            raise ValueError(f"Checkpoint must contain 'codebooks' key. Found keys: {checkpoint.keys()}")
        
        print(f"Loaded PQ codebook: {self.num_subspaces} subspaces, each {self.subspace_dim}D, {self.codebook_size} entries per subspace")
    
    def quantize(self, embeddings):
        """
        Product Quantization: å°†embeddingsåˆ†æˆå¤šä¸ªå­ç©ºé—´ï¼Œæ¯ä¸ªå­ç©ºé—´ç‹¬ç«‹é‡åŒ–
        embeddings: (batch, seq_len, emb_dim)
        è¿”å›: (batch, seq_len, emb_dim) é‡åŒ–åçš„embeddings, (batch, seq_len, num_subspaces) æ¯ä¸ªå­ç©ºé—´çš„ç´¢å¼•
        """
        batch_size, seq_len, emb_dim = embeddings.shape
        flat_embs = embeddings.view(-1, emb_dim)  # (batch * seq_len, emb_dim)
        
        # å°†embeddingsåˆ†æˆå­ç©ºé—´
        subspace_embs = flat_embs.view(-1, self.num_subspaces, self.subspace_dim)
        
        quantized_parts = []
        all_indices = []
        
        # å¯¹æ¯ä¸ªå­ç©ºé—´ç‹¬ç«‹é‡åŒ–
        for i, codebook in enumerate(self.codebooks):
            subspace = subspace_embs[:, i, :]  # (batch * seq_len, subspace_dim)
            
            # ç¡®ä¿codebookåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            codebook = codebook.to(subspace.device)
            
            # è®¡ç®—è·ç¦»: (batch * seq_len, codebook_size)
            distances = torch.cdist(subspace, codebook, p=2)
            
            # æ‰¾åˆ°æœ€è¿‘é‚»ç´¢å¼•
            indices = torch.argmin(distances, dim=-1)  # (batch * seq_len,)
            
            # ä»codebookä¸­è·å–é‡åŒ–åçš„embeddings
            quantized = codebook[indices]  # (batch * seq_len, subspace_dim)
            
            quantized_parts.append(quantized)
            all_indices.append(indices)
        
        # æ‹¼æ¥æ‰€æœ‰å­ç©ºé—´çš„é‡åŒ–ç»“æœ
        quantized = torch.cat(quantized_parts, dim=-1)  # (batch * seq_len, emb_dim)
        quantized = quantized.view(batch_size, seq_len, emb_dim)
        
        # æ‰€æœ‰å­ç©ºé—´çš„ç´¢å¼•: (batch * seq_len, num_subspaces)
        all_indices = torch.stack(all_indices, dim=-1)
        all_indices = all_indices.view(batch_size, seq_len, self.num_subspaces)
        
        return quantized, all_indices
    
    def forward(self, embeddings):
        """é‡åŒ–embeddingsï¼ˆä¸è®­ç»ƒï¼Œåªç”¨äºæ¨ç†ï¼‰"""
        quantized, indices = self.quantize(embeddings)
        return quantized, indices


class MLPProjection(nn.Module):
    """MLPæŠ•å½±å±‚ï¼šå°†é‡åŒ–åçš„embeddingsæŠ•å½±åˆ°LLMç»´åº¦"""
    def __init__(self, input_dim=1024, hidden_dim=None, output_dim=4096):
        super().__init__()
        if hidden_dim is None:
            hidden_dim = output_dim  # é»˜è®¤ä½¿ç”¨output_dimä½œä¸ºhidden_dim
        
        # ä¸¤å±‚MLP: input_dim -> hidden_dim -> output_dim
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        """
        x: (batch, seq_len, input_dim)
        è¿”å›: (batch, seq_len, output_dim)
        """
        return self.mlp(x)


def mean_pooling(token_embeddings, attention_mask):
    """Mean pooling for sentence embeddings"""
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    return sum_embeddings / sum_mask


def get_query_LaMP_7(inp):
    """ä» input ä¸­æå–æŸ¥è¯¢æ–‡æœ¬ï¼ˆLaMP-7æ ¼å¼ï¼‰"""
    substr = "before or after it: "
    plc = inp.find(substr)
    if plc == -1:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œè¿”å›æ•´ä¸ªinput
        return inp.strip()
    query = inp[plc + len(substr):].strip()
    return query


def ensure_chat_template(tokenizer, model_path):
    """
    ç¡®ä¿tokenizeræœ‰chat_templateï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»æ–‡ä»¶ä¸­åŠ è½½
    """
    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None:
        return
    
    # å°è¯•ä»æ–‡ä»¶ä¸­åŠ è½½chat_template
    for name in ("chat_template.jinja", "chat_template.txt", "chat_template.json"):
        template_path = os.path.join(model_path, name)
        if os.path.exists(template_path):
            try:
                with open(template_path, "r", encoding="utf-8") as f:
                    tokenizer.chat_template = f.read()
                print(f"âœ… ä»æ–‡ä»¶åŠ è½½ chat_template: {template_path}")
                return
            except Exception as e:
                print(f"âš ï¸  è¯»å– chat_template å¤±è´¥: {template_path} -> {e}")


def encode_text_with_encoder(text, encoder_model, encoder_tokenizer, device):
    """ä½¿ç”¨encoderå®æ—¶ç¼–ç æ–‡æœ¬"""
    encoder_model.eval()
    
    with torch.no_grad():
        inputs = encoder_tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        ).to(device)
        
        outputs = encoder_model(**inputs)
        token_embeddings = outputs[0]  # (1, seq_len, hidden_size)
        
        # Mean pooling
        embeddings = mean_pooling(token_embeddings, inputs['attention_mask'])
        
        # L2å½’ä¸€åŒ–ï¼ˆcontrieveré€šå¸¸éœ€è¦å½’ä¸€åŒ–ï¼‰
        embeddings = F.normalize(embeddings, p=2, dim=1)
        
        return embeddings.squeeze(0)  # (1024,)


class LaMP7InferenceDataset(Dataset):
    """LaMP-7æ¨ç†æ•°æ®é›†ï¼šä»JSONè¯»å–ï¼Œå®æ—¶ç¼–ç profile"""
    def __init__(self, json_path, encoder_model, encoder_tokenizer, llm_tokenizer, 
                 his_len=8, device="cpu"):
        self.encoder_model = encoder_model
        self.encoder_tokenizer = encoder_tokenizer
        self.llm_tokenizer = llm_tokenizer
        self.his_len = his_len
        self.device = device
        
        # åŠ è½½æ•°æ®
        print(f"Loading data from {json_path}...")
        with open(json_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"Loaded {len(self.data)} data samples")
        
        # å ä½ç¬¦token
        self.placeholder_token = "<USR_EMB>"
        # ç¡®ä¿å ä½ç¬¦åœ¨LLMçš„tokenizerè¯è¡¨ä¸­
        if self.placeholder_token not in llm_tokenizer.get_vocab():
            llm_tokenizer.add_tokens([self.placeholder_token])
            print(f"Added placeholder token to LLM tokenizer: {self.placeholder_token}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        item_id = item.get("id", "")  # æ ·æœ¬ID
        input_text = item.get("input", "")  # éœ€è¦æ”¹å†™çš„æ¨æ–‡è¾“å…¥
        profile = item.get("profile", [])  # ç”¨æˆ·å†å²profile
        
        # æ„å»ºprofileæ–‡æœ¬å¹¶ç¼–ç ï¼ˆLaMP-7æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨textå­—æ®µï¼‰
        user_embs_list = []
        for i, profile_item in enumerate(profile[:self.his_len]):
            text = profile_item.get("text", "")
            
            # LaMP-7: ç›´æ¥ä½¿ç”¨textï¼Œä¸éœ€è¦ç»„åˆtitle
            if text.strip():
                # ç¼–ç 
                emb = encode_text_with_encoder(text, self.encoder_model, self.encoder_tokenizer, self.device)
                user_embs_list.append(emb.cpu())
            else:
                # å¦‚æœtextä¸ºç©ºï¼Œä½¿ç”¨é›¶å‘é‡
                user_embs_list.append(torch.zeros(1024))
        
        # å¦‚æœä¸å¤Ÿï¼Œç”¨é›¶å‘é‡å¡«å……
        while len(user_embs_list) < self.his_len:
            user_embs_list.append(torch.zeros(1024))
        
        # å †å æˆtensor: (his_len, 1024)
        user_embs = torch.stack(user_embs_list[:self.his_len])
        
        return {
            "id": item_id,
            "input": input_text,  # éœ€è¦æ”¹å†™çš„æ¨æ–‡è¾“å…¥
            "user_embeddings": user_embs  # (his_len, 1024)
        }


def load_models(codebook_path, mlp_path, llm_path, encoder_path, device="cuda:0"):
    """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
    print(f"Loading models on {device}...")
    
    # åŠ è½½encoder
    print(f"  Loading encoder from {encoder_path}...")
    encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_path, trust_remote_code=True)
    encoder_model = AutoModel.from_pretrained(encoder_path, trust_remote_code=True)
    encoder_model.to(device)
    encoder_model.eval()
    
    # åŠ è½½PQ codebook
    print(f"  Loading PQ codebook from {codebook_path}...")
    pq_codebook_model = PQCodebookModel(codebook_path, device=device)
    pq_codebook_model.eval()
    
    # åŠ è½½MLP
    print(f"  Loading MLP from {mlp_path}...")
    mlp_checkpoint = torch.load(mlp_path, map_location=device)
    mlp_model = MLPProjection(
        input_dim=mlp_checkpoint["input_dim"],
        hidden_dim=mlp_checkpoint["hidden_dim"],
        output_dim=mlp_checkpoint["output_dim"]
    )
    mlp_model.load_state_dict(mlp_checkpoint["mlp"])
    mlp_model.to(device)
    mlp_model.eval()
    
    # åŠ è½½LLM
    print(f"  Loading LLM from {llm_path}...")
    tokenizer = AutoTokenizer.from_pretrained(llm_path, trust_remote_code=True)
    ensure_chat_template(tokenizer, llm_path)
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    llm_model = AutoModelForCausalLM.from_pretrained(
        llm_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map={"": device}
    )
    llm_model.eval()
    
    # ç¡®ä¿å ä½ç¬¦åœ¨tokenizerä¸­
    placeholder_token = "<USR_EMB>"
    if placeholder_token not in tokenizer.get_vocab():
        tokenizer.add_tokens([placeholder_token])
        print(f"  Added placeholder token: {placeholder_token}")
    
    print("âœ… All models loaded!")
    return encoder_model, encoder_tokenizer, pq_codebook_model, mlp_model, llm_model, tokenizer


def inference_batch(batch, encoder_model, encoder_tokenizer, pq_codebook_model, mlp_model, 
                   llm_model, tokenizer, his_len=8, max_new_tokens=128, device="cuda:0"):
    """
    å¯¹batchè¿›è¡Œæ¨ç†
    
    Args:
        batch: åŒ…å«id, input, user_embeddingsçš„å­—å…¸
        encoder_model: Encoderæ¨¡å‹
        encoder_tokenizer: Encoderçš„tokenizer
        pq_codebook_model: PQ codebookæ¨¡å‹
        mlp_model: MLPæŠ•å½±æ¨¡å‹
        llm_model: LLMæ¨¡å‹
        tokenizer: LLMçš„tokenizer
        his_len: å†å²é•¿åº¦
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        device: è®¾å¤‡
    
    Returns:
        generated_texts: ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
    """
    placeholder_token = "<USR_EMB>"
    inputs = batch["input"]
    user_embeddings = batch["user_embeddings"].to(device)  # (batch_size, his_len, 1024)
    batch_size = len(inputs)
    
    # PQé‡åŒ–ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
    with torch.no_grad():
        quantized_embs, _ = pq_codebook_model(user_embeddings)  # (batch_size, his_len, 1024)
    
    # MLPæŠ•å½±åˆ°LLMç»´åº¦ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
    with torch.no_grad():
        llm_embs = mlp_model(quantized_embs)  # (batch_size, his_len, llm_dim)
    
    # è·å–LLMçš„embeddingå±‚å’Œæ•°æ®ç±»å‹
    llm_embeddings = llm_model.get_input_embeddings()
    model_dtype = next(llm_model.parameters()).dtype
    
    # ç¡®ä¿llm_embsä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
    llm_embs = llm_embs.to(dtype=model_dtype)
    
    # æ„å»ºæ¯ä¸ªæ ·æœ¬çš„promptå¹¶tokenize
    input_ids_list = []
    placeholder_positions_list = []
    
    placeholder_id = tokenizer.convert_tokens_to_ids(placeholder_token)
    
    for i in range(batch_size):
        input_text = inputs[i]
        
        # ä»inputä¸­æå–æŸ¥è¯¢æ–‡æœ¬ï¼ˆLaMP-7æ ¼å¼ï¼‰
        query_text = get_query_LaMP_7(input_text)
        
        # æ„å»ºpromptï¼ˆä½¿ç”¨soft promptæ–¹å¼ï¼Œä¸åœ¨æ–‡æœ¬ä¸­æ’å…¥å ä½ç¬¦ï¼‰
        if his_len > 0:
            # ä½¿ç”¨å•ä¸ªå ä½ç¬¦æ ‡è®°æ¥å®šä½ï¼Œåç»­ä¼šç”¨his_lenä¸ªsoft tokensæ›¿æ¢
            user_prompt_text = (
                f"User style embedding: {placeholder_token}\n"
                "Based on the user's style embedding provided above, please paraphrase the user's input tweet without any explanation before or after it.\n"
                f"{query_text}"
            )
        else:
            user_prompt_text = (
                "Please paraphrase the user's input tweet without any explanation before or after it.\n"
                "Please generate it in the following format: {'tweet': 'generated tweet'} without explanation, and use only English.\n"
                f"{query_text}"
            )
        
        # ä½¿ç”¨chat template
        messages = [{"role": "user", "content": user_prompt_text}]
        
        # åº”ç”¨chat template
        if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None:
            formatted = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # Fallback: æ‰‹åŠ¨æ„å»ºQwenæ ¼å¼
            formatted = f"<|im_start|>user\n{user_prompt_text}<|im_end|>\n<|im_start|>assistant\n"
        
        # Tokenize
        encoded = tokenizer(
            formatted,
            return_tensors="pt",
            padding=False,
            truncation=True,
            max_length=512,
            add_special_tokens=False
        )
        
        input_ids = encoded["input_ids"].squeeze(0)  # (seq_len,)
        
        # æ‰¾åˆ°å ä½ç¬¦çš„ä½ç½®
        placeholder_positions = (input_ids == placeholder_id).nonzero(as_tuple=True)[0].tolist()
        
        if len(placeholder_positions) > 0 and his_len > 0:
            insert_start_pos = placeholder_positions[0]
            placeholder_positions_list.append((insert_start_pos, his_len))
        else:
            placeholder_positions_list.append((None, 0))
        
        input_ids_list.append(input_ids)
    
    # Paddingï¼ˆleft paddingï¼Œå› ä¸ºLLMæ˜¯decoder-onlyï¼‰
    max_seq_len = max(ids.size(0) for ids in input_ids_list)
    
    padded_input_ids = []
    attention_masks = []
    
    for input_ids in input_ids_list:
        seq_len = input_ids.size(0)
        padding_length = max_seq_len - seq_len
        
        # Left padding
        padded_input = torch.cat([
            torch.full((padding_length,), tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id, dtype=input_ids.dtype),
            input_ids
        ])
        padded_input_ids.append(padded_input)
        
        # Attention mask
        attention_mask = torch.cat([
            torch.zeros(padding_length, dtype=torch.long),
            torch.ones(seq_len, dtype=torch.long)
        ])
        attention_masks.append(attention_mask)
    
    input_ids_batch = torch.stack(padded_input_ids).to(device)  # (batch_size, max_seq_len)
    attention_mask_batch = torch.stack(attention_masks).to(device)  # (batch_size, max_seq_len)
    
    # è·å–LLMçš„embeddingå±‚
    input_embs = llm_embeddings(input_ids_batch)  # (batch_size, max_seq_len, llm_dim)
    
    # æ›¿æ¢å ä½ç¬¦ä½ç½®çš„embeddingsï¼ˆä½¿ç”¨soft promptæ–¹å¼ï¼‰
    for i in range(batch_size):
        insert_start_pos, num_tokens = placeholder_positions_list[i]
        if insert_start_pos is not None and num_tokens > 0:
            # è°ƒæ•´ä½ç½®ï¼ˆå› ä¸ºleft paddingï¼‰
            padding_length = max_seq_len - input_ids_list[i].size(0)
            adjusted_start_pos = insert_start_pos + padding_length
            
            # åœ¨å ä½ç¬¦ä½ç½®æ’å…¥his_lenä¸ªsoft tokens
            if adjusted_start_pos + num_tokens <= max_seq_len:
                # ç›´æ¥æ›¿æ¢å ä½ç¬¦åŠå…¶åç»­ä½ç½®çš„embeddings
                for j in range(num_tokens):
                    if adjusted_start_pos + j < max_seq_len:
                        input_embs[i, adjusted_start_pos + j] = llm_embs[i, j]
            else:
                # å¦‚æœä½ç½®è¶…å‡ºï¼Œåªæ›¿æ¢èƒ½æ”¾ä¸‹çš„éƒ¨åˆ†
                available_slots = max_seq_len - adjusted_start_pos
                for j in range(min(available_slots, num_tokens)):
                    input_embs[i, adjusted_start_pos + j] = llm_embs[i, j]
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = llm_model.generate(
            inputs_embeds=input_embs,
            attention_mask=attention_mask_batch,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç ç”Ÿæˆç»“æœ
    generated_texts = []
    for i, output_ids in enumerate(outputs):
        # æ‰¾åˆ°è¾“å…¥éƒ¨åˆ†çš„é•¿åº¦
        input_len = input_ids_list[i].size(0)
        # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰paddingï¼‰
        padding_length = max_seq_len - input_len
        gen = output_ids
        pred = tokenizer.decode(gen, skip_special_tokens=True).strip()
        generated_texts.append(pred)
    
    return generated_texts


def main():
    parser = argparse.ArgumentParser(description="LaMP-7 Inference: MLP with PQ Codebook")
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--codebook_path", type=str, required=True,
                       help="è®­ç»ƒå¥½çš„PQ codebookæ¨¡å‹è·¯å¾„")
    parser.add_argument("--mlp_path", type=str, required=True,
                       help="è®­ç»ƒå¥½çš„MLPæ¨¡å‹è·¯å¾„")
    parser.add_argument("--llm_path", type=str, required=True,
                       help="LLMæ¨¡å‹è·¯å¾„")
    parser.add_argument("--encoder_path", type=str, required=True,
                       help="Encoderæ¨¡å‹è·¯å¾„ï¼ˆContrieverï¼‰")
    
    # æ•°æ®è·¯å¾„
    parser.add_argument("--questions_path", type=str, required=True,
                       help="æµ‹è¯•é—®é¢˜JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--outputs_path", type=str, default=None,
                       help="Ground truthè¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºè¯„ä¼°ï¼‰")
    parser.add_argument("--output_path", type=str, required=True,
                       help="é¢„æµ‹ç»“æœä¿å­˜è·¯å¾„")
    
    # æ¨ç†å‚æ•°
    parser.add_argument("--his_len", type=int, default=8,
                       help="Number of historical profile items to use")
    parser.add_argument("--batch_size", type=int, default=4,
                       help="Batch size")
    parser.add_argument("--max_new_tokens", type=int, default=128,
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--device", type=str, default="cuda:0",
                       help="è®¾å¤‡ï¼ˆcuda:0, cuda:1, cpuç­‰ï¼‰")
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("LaMP-7 Inference: MLP with PQ Codebook")
    print("=" * 80)
    print(f"Codebook: {args.codebook_path}")
    print(f"MLP: {args.mlp_path}")
    print(f"LLM: {args.llm_path}")
    print(f"Encoder: {args.encoder_path}")
    print(f"Questions: {args.questions_path}")
    print(f"Output: {args.output_path}")
    print(f"History length: {args.his_len}")
    print(f"Batch size: {args.batch_size}")
    print(f"Device: {args.device}")
    print("=" * 80)
    
    # åŠ è½½æ¨¡å‹
    encoder_model, encoder_tokenizer, pq_codebook_model, mlp_model, llm_model, tokenizer = load_models(
        codebook_path=args.codebook_path,
        mlp_path=args.mlp_path,
        llm_path=args.llm_path,
        encoder_path=args.encoder_path,
        device=args.device
    )
    
    # åŠ è½½ground truthï¼ˆå¦‚æœæœ‰ï¼‰
    golds = {}
    if args.outputs_path:
        print(f"\nLoading ground truth from {args.outputs_path}...")
        with open(args.outputs_path, 'r', encoding='utf-8') as f:
            outputs_data = json.load(f)
        if "golds" in outputs_data:
            golds = {item['id']: item['output'] for item in outputs_data['golds']}
        else:
            for item in outputs_data:
                item_id = item.get("id", "")
                output = item.get("output", "")
                if item_id:
                    golds[item_id] = output
        print(f"âœ… åŠ è½½ {len(golds)} æ¡ground truth")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = LaMP7InferenceDataset(
        json_path=args.questions_path,
        encoder_model=encoder_model,
        encoder_tokenizer=encoder_tokenizer,
        llm_tokenizer=tokenizer,
        his_len=args.his_len,
        device=args.device
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0  # å› ä¸ºéœ€è¦å®æ—¶ç¼–ç ï¼Œæ‰€ä»¥ä¸ç”¨å¤šè¿›ç¨‹
    )
    
    # æ¨ç†
    print(f"\nå¼€å§‹æ¨ç†...")
    predictions = []
    
    for batch in tqdm(dataloader, desc="ğŸ§  æ¨ç†"):
        generated_texts = inference_batch(
            batch=batch,
            encoder_model=encoder_model,
            encoder_tokenizer=encoder_tokenizer,
            pq_codebook_model=pq_codebook_model,
            mlp_model=mlp_model,
            llm_model=llm_model,
            tokenizer=tokenizer,
            his_len=args.his_len,
            max_new_tokens=args.max_new_tokens,
            device=args.device
        )
        
        # ä¿å­˜ç»“æœ
        for i, pred_text in enumerate(generated_texts):
            item_id = batch["id"][i]
            predictions.append({
                'id': item_id,
                'input': batch["input"][i],
                'prediction': pred_text,
                'gold': golds.get(item_id, '') if golds else ''
            })
    
    # ä¿å­˜ç»“æœ
    print(f"\næ­£åœ¨ä¿å­˜ç»“æœåˆ°: {args.output_path}")
    os.makedirs(os.path.dirname(args.output_path) if os.path.dirname(args.output_path) else '.', exist_ok=True)
    
    with open(args.output_path, 'w', encoding='utf-8') as f:
        json.dump(predictions, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ¨ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(predictions)} æ¡é¢„æµ‹ç»“æœ")
    print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {args.output_path}")
    
    # å¦‚æœæœ‰ground truthï¼Œæ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    if golds:
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(predictions)}")
        print(f"   æœ‰ground truthçš„æ ·æœ¬æ•°: {len(golds)}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªé¢„æµ‹ç»“æœç¤ºä¾‹
        print(f"\nğŸ“ é¢„æµ‹ç»“æœç¤ºä¾‹ï¼ˆå‰3æ¡ï¼‰:")
        for i, pred in enumerate(predictions[:3]):
            print(f"\n   [{i+1}] ID: {pred['id']}")
            print(f"       è¾“å…¥: {pred['input'][:100]}...")
            print(f"       é¢„æµ‹: {pred['prediction'][:100]}...")
            if pred['gold']:
                print(f"      çœŸå®: {pred['gold'][:100]}...")


if __name__ == "__main__":
    main()

